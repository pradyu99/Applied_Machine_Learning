{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "HOMEWORK 4\n",
        "\n",
        "VARSHA KARANAM\n",
        "\n",
        "VEDA CHARITHA BELLAM\n",
        "\n",
        "PRADYUMNA KAPUTHIMAREDDY\n",
        "\n",
        "\n",
        "\n",
        "1)Create your own dataset for text classification. It should contain at least 2000 words in total and at least three categories with at least 100 examples per category (an example can be a poem or a paragraph from a book). You can create it by scraping the web or using some of the documents you have on your computer (do not use anything confidential) or ChatGPT."
      ],
      "metadata": {
        "id": "590EczYf7F8F"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "aXHWGE05Wlxc"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rKzJ6dF12Erp",
        "outputId": "5cc2e278-bb74-4f24-d292-157c91e597e9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset created successfully!\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "\n",
        "categories = ['Nature', 'Technology', 'Literature']\n",
        "nature_examples = [\n",
        "    \"The gentle rustle of leaves in the breeze whispered tales of ancient forests and forgotten legends to those who listened.\",\n",
        "    \"As dawn broke, the first rays of sunlight kissed the dew-covered grass, painting the meadow in a golden hue.\",\n",
        "    \"Butterflies flitted gracefully from flower to flower, their delicate wings shimmering in the morning light.\",\n",
        "    \"The tranquil melody of a babbling brook echoed through the valley, soothing the weary souls of travelers.\",\n",
        "    \"Beneath the canopy of stars, the nocturnal creatures of the forest danced to the rhythm of the night, their silhouettes weaving intricate patterns in the darkness.\",\n",
        "    \"A gentle rain shower bathed the earth in a cleansing embrace, nourishing the soil and awakening dormant seeds.\",\n",
        "    \"The majestic oak tree stood tall and proud, its branches reaching towards the heavens in silent reverence.\",\n",
        "    \"Fields of vibrant wildflowers stretched as far as the eye could see, painting the landscape in a kaleidoscope of colors.\",\n",
        "    \"A family of deer grazed peacefully in the meadow, their graceful movements adding to the serene beauty of the landscape.\",\n",
        "    \"The scent of pine trees hung heavy in the air, mingling with the sweet aroma of wildflowers, as the forest whispered secrets of times long past.\"\n",
        "]\n",
        "\n",
        "technology_examples = [\n",
        "    \"In the bustling city streets, neon lights illuminated the faces of passersby, casting an otherworldly glow upon the urban landscape.\",\n",
        "    \"The whirring of gears and the rhythmic clanking of machinery filled the air of the industrial district, a symphony of progress and innovation.\",\n",
        "    \"Holographic displays flickered to life, projecting holograms of futuristic cities and advanced technology into the room.\",\n",
        "    \"Programmers meticulously crafted lines of code, each keystroke bringing their digital creations one step closer to reality.\",\n",
        "    \"Autonomous vehicles zipped through the cityscape, navigating the bustling streets with precision and efficiency, a testament to human ingenuity.\",\n",
        "    \"The soft glow of a computer screen illuminated the dimly lit room, as a programmer worked tirelessly into the night, bringing their vision to life.\",\n",
        "    \"Nanobots swarmed through the bloodstream, repairing damaged tissue and fighting off infections with pinpoint accuracy.\",\n",
        "    \"The hum of drones filled the air as they soared overhead, capturing breathtaking aerial footage of the cityscape, their cameras recording every detail with stunning clarity.\",\n",
        "    \"Artificial intelligence algorithms analyzed vast datasets, uncovering hidden patterns and insights that eluded human comprehension.\",\n",
        "    \"Quantum computers revolutionized the field of computing, solving complex problems with unprecedented speed and efficiency, unlocking new possibilities in fields ranging from cryptography to drug discovery.\"\n",
        "]\n",
        "\n",
        "literature_examples = [\n",
        "    \"In the heart of the enchanted forest, a young heroine embarked on a quest to unravel the mysteries of an ancient prophecy and save her kingdom from darkness.\",\n",
        "    \"\\\"It was the best of times, it was the worst of times,\\\" began the timeless tale of love, sacrifice, and redemption set against the backdrop of revolution.\",\n",
        "    \"The haunting melody of a violin echoed through the halls of the abandoned mansion, a ghostly reminder of the tragic love story that unfolded within its walls.\",\n",
        "    \"Amidst the ruins of a once-great empire, a band of rebels rose up against tyranny, their rallying cry echoing across the desolate landscape.\",\n",
        "    \"From the ashes of war, a new era of peace and prosperity dawned, as heroes and heroines alike forged a brighter future for generations to come.\",\n",
        "    \"The pages of an ancient tome crackled with magic as a young wizard delved into its secrets, seeking knowledge that would change the course of history.\",\n",
        "    \"A mysterious stranger wandered into town, his cloak billowing behind him as he whispered tales of adventure and intrigue to anyone who would listen.\",\n",
        "    \"In the depths of the ocean, a mermaid sang a haunting melody, her voice luring sailors to their doom as they searched for the lost city of Atlantis.\",\n",
        "    \"The castle loomed ominously on the hill, its darkened windows hiding secrets of betrayal and revenge that spanned centuries.\",\n",
        "    \"As the sun set on the horizon, a lone figure stood on the cliff's edge, gazing out at the endless expanse of the sea, lost in thoughts of love and loss.\"\n",
        "]\n",
        "\n",
        "\n",
        "\n",
        "data = pd.DataFrame(columns=['Text', 'Category'])\n",
        "\n",
        "\n",
        "for category, examples in zip(categories, [nature_examples, technology_examples, literature_examples]):\n",
        "    for example in examples:\n",
        "        data = pd.concat([data, pd.DataFrame({'Text': [example], 'Category': [category]})], ignore_index=True)\n",
        "\n",
        "\n",
        "data = data.sample(frac=1).reset_index(drop=True)\n",
        "\n",
        "\n",
        "data.to_csv('text_dataset.csv', index=False)\n",
        "\n",
        "print(\"Dataset created successfully!\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code creates a synthetic dataset of text examples categorized into 'Nature', 'Technology', and 'Literature'. It defines example sentences for each category, creates a DataFrame to store the data, populates the DataFrame with examples, shuffles the dataset, saves it to a CSV file."
      ],
      "metadata": {
        "id": "J9zD1n9zVutW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "\n",
        "min_train_examples = 240\n",
        "min_test_examples = 60\n",
        "\n",
        "train_data, test_data = train_test_split(data, test_size=0.2, stratify=data['Category'])\n",
        "\n",
        "\n",
        "train_data = train_data.groupby('Category').apply(lambda x: x.sample(min(min_train_examples, len(x)))).reset_index(drop=True)\n",
        "\n",
        "\n",
        "test_data = test_data.groupby('Category').apply(lambda x: x.sample(min(min_test_examples, len(x)))).reset_index(drop=True)\n",
        "\n",
        "\n",
        "train_data = train_data.sample(frac=1).reset_index(drop=True)\n",
        "test_data = test_data.sample(frac=1).reset_index(drop=True)\n",
        "\n",
        "\n",
        "train_data.to_csv('train_dataset.csv', index=False)\n",
        "test_data.to_csv('test_dataset.csv', index=False)\n",
        "\n",
        "print(\"Training and test datasets created successfully!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q1DIPSGs2nWs",
        "outputId": "4e469bc0-02f9-4cfe-8c1d-ee76f6859b9b"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training and test datasets created successfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import AdamW\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "\n",
        "\n",
        "model_name = \"gpt2\"\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "model = GPT2LMHeadModel.from_pretrained(model_name)\n",
        "\n",
        "\n",
        "class CustomDataset(Dataset):\n",
        "    def __init__(self, tokenizer, data_file, block_size):\n",
        "        self.examples = []\n",
        "        with open(data_file, \"r\") as f:\n",
        "            lines = f.readlines()\n",
        "            for line in lines:\n",
        "                self.examples.append(tokenizer.encode(line.strip()))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.examples)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return torch.tensor(self.examples[idx])\n",
        "\n",
        "\n",
        "num_train_epochs = 11\n",
        "learning_rate = 5e-5\n",
        "batch_size = 4\n",
        "\n",
        "\n",
        "train_dataset = CustomDataset(tokenizer, \"train_dataset.csv\", block_size=128)\n",
        "\n",
        "\n",
        "train_dataset_tensors = [torch.tensor(example) for example in train_dataset.examples]\n",
        "\n",
        "padded_train_dataset = pad_sequence(train_dataset_tensors, batch_first=True, padding_value=tokenizer.pad_token_id)\n",
        "\n",
        "\n",
        "train_loader = DataLoader(padded_train_dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "optimizer = AdamW(model.parameters(), lr=learning_rate)\n",
        "\n",
        "for epoch in range(num_train_epochs):\n",
        "    model.train()\n",
        "    total_loss = 0.0\n",
        "    for batch in train_loader:\n",
        "        inputs = batch.to(device)\n",
        "        outputs = model(inputs, labels=inputs)\n",
        "        loss = outputs.loss\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()\n",
        "        total_loss += loss.item()\n",
        "    print(f\"Epoch {epoch + 1} Loss: {total_loss}\")\n",
        "\n",
        "\n",
        "model.save_pretrained(\"gpt2-finetuned\")\n",
        "\n",
        "print(\"Fine-tuning completed!\")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CIB80_-x4H9z",
        "outputId": "996d80ba-8670-4ad4-f10a-146245dd0e5c"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:521: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 Loss: 30.896241903305054\n",
            "Epoch 2 Loss: 15.595089435577393\n",
            "Epoch 3 Loss: 12.065539956092834\n",
            "Epoch 4 Loss: 10.006148219108582\n",
            "Epoch 5 Loss: 7.617909789085388\n",
            "Epoch 6 Loss: 6.397918164730072\n",
            "Epoch 7 Loss: 5.187341272830963\n",
            "Epoch 8 Loss: 3.8969529569149017\n",
            "Epoch 9 Loss: 2.973611533641815\n",
            "Epoch 10 Loss: 2.4432801604270935\n",
            "Epoch 11 Loss: 2.0208991318941116\n",
            "Fine-tuning completed!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code fine-tunes a pre-trained GPT-2 language model on a custom dataset by first loading and preprocessing the training data, then creating a DataLoader for efficient batch processing. The model undergoes training for a specified number of epochs, where each iteration involves computing the loss between model predictions and actual data, backpropagating gradients, and updating model parameters using the AdamW optimizer. After training, the fine-tuned model is saved to disk, enabling its use for various natural language processing tasks such as text generation or completion."
      ],
      "metadata": {
        "id": "jGy_vMa4WIj9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We used perplexity as accuracy metric. Test perplexity is a measure of how well a language model predicts sequences of tokens (like words or characters) on unseen data. Lower perplexity indicates better performance, meaning the model's predictions align more closely with the actual data. It's calculated as the exponentiation of the average cross-entropy loss, with higher values suggesting greater uncertainty or less accurate predictions."
      ],
      "metadata": {
        "id": "Gq8-0WWGUiF1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "test_dataset = CustomDataset(tokenizer, \"test_dataset.csv\", block_size=128)\n",
        "\n",
        "\n",
        "test_dataset_tensors = [torch.tensor(example) for example in test_dataset.examples]\n",
        "\n",
        "\n",
        "padded_test_dataset = pad_sequence(test_dataset_tensors, batch_first=True, padding_value=tokenizer.pad_token_id)\n",
        "\n",
        "\n",
        "test_loader = DataLoader(padded_test_dataset, batch_size=batch_size)\n",
        "\n",
        "\n",
        "model.eval()\n",
        "total_loss = 0.0\n",
        "num_batches = 0\n",
        "\n",
        "with torch.no_grad():\n",
        "    for batch in test_loader:\n",
        "        inputs = batch.to(device)\n",
        "        outputs = model(inputs, labels=inputs)\n",
        "        loss = outputs.loss\n",
        "        total_loss += loss.item()\n",
        "        num_batches += 1\n",
        "\n",
        "\n",
        "test_perplexity = torch.exp(torch.tensor(total_loss / num_batches))\n",
        "print(\"Test Perplexity:\", test_perplexity.item())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VHbOgPAd49TZ",
        "outputId": "05f994c7-fab1-43df-fde6-9dbc44a9df4f"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Perplexity: 13.89050579071045\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The test perplexity is 13.8905\n",
        "\n",
        "To increase the accuracy we can perform the following steps:\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "*   Grid search or random search over hyperparameters might help find better combinations.\n",
        "*   Instead of using a fixed learning rate, try scheduling the learning rate to decrease over time. Techniques like learning rate warm-up or using a learning rate scheduler can sometimes improve convergence.\n",
        "\n",
        "*   Explore different loss functions or modify the existing loss function to better suit the task or model architecture.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ua728kGmUv9x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3YtONeaC3mQQ",
        "outputId": "9d2705f3-3668-4a06-f833-0b32e429967e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.40.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.13.4)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.20.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.25.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.12.25)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.2)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (4.11.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.2.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers accelerate\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hyLZ1O133Gno",
        "outputId": "817b4f99-24a6-4c9f-d4d2-acafcdec4c49"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.40.0)\n",
            "Collecting accelerate\n",
            "  Downloading accelerate-0.29.3-py3-none-any.whl (297 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m297.6/297.6 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.13.4)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.20.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.25.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.12.25)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.2)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (2.2.1+cu121)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (4.11.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.1.3)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch>=1.10.0->accelerate)\n",
            "  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch>=1.10.0->accelerate)\n",
            "  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch>=1.10.0->accelerate)\n",
            "  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch>=1.10.0->accelerate)\n",
            "  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch>=1.10.0->accelerate)\n",
            "  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch>=1.10.0->accelerate)\n",
            "  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch>=1.10.0->accelerate)\n",
            "  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch>=1.10.0->accelerate)\n",
            "  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch>=1.10.0->accelerate)\n",
            "  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "Collecting nvidia-nccl-cu12==2.19.3 (from torch>=1.10.0->accelerate)\n",
            "  Using cached nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl (166.0 MB)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from torch>=1.10.0->accelerate)\n",
            "  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "Requirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2.2.0)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.10.0->accelerate)\n",
            "  Using cached nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.2.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.5)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n",
            "Installing collected packages: nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, accelerate\n",
            "Successfully installed accelerate-0.29.3 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.19.3 nvidia-nvjitlink-cu12-12.4.127 nvidia-nvtx-cu12-12.1.105\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "REFERENCES:\n",
        "\n",
        "https://huggingface.co/openai-community/gpt2\n",
        "\n",
        "https://towardsdatascience.com/natural-language-generation-part-2-gpt-2-and-huggingface-f3acb35bc86a\n",
        "\n",
        "CHATGPT"
      ],
      "metadata": {
        "id": "ZgEZCpNlWLdk"
      }
    }
  ]
}